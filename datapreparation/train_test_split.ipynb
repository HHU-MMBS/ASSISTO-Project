{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime, timedelta\n",
    "from astropy.table import QTable, Table, Column\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(6)\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"path to input excel sheet\") \n",
    "data_sheet1 = xls.parse(0)\n",
    "data_sheet1= data_sheet1[:1913]\n",
    "data_sheet1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting  patients' IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_id = data_sheet1[\"Pat-ID\"]\n",
    "print(np.unique(pat_id.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_regular = data_sheet1[\"Regular day_infectious [1 = 'regular day', 0 = 'irregular day']\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting date and making a dictionary  with key=[date, ID] and value=is_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = data_sheet1[\"Date\"]\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {}\n",
    "for i in range (len(date)):\n",
    "    Dict[str([date[i].date(), pat_id[i]])]= is_regular[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Cohort [0 = Inpatient, 1 = Outpatient] and making a dictionary  with key=[date, ID] and value=cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = data_sheet1[\"Cohort [0 = Inpatient, 1 = Outpatient]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_cohort = {}\n",
    "for i in range (len(date)):\n",
    "    Dict_cohort[str([date[i].date(), pat_id[i]])]= cohort[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_samples(all_intervals):\n",
    "    c = 0\n",
    "    for i in range(len(all_intervals)):\n",
    "        c +=1\n",
    "    return c\n",
    "\n",
    "def max_min_len(all_intervals):\n",
    "    length_lst = []\n",
    "    for i in range(len(all_intervals)):\n",
    "        length_lst.append((all_intervals[i][0]).shape[0])\n",
    "    return  [min(length_lst), max(length_lst)] \n",
    "\n",
    "def len_samples(all_intervals):\n",
    "    length_lst = []\n",
    "    for i in range(len(all_intervals)):\n",
    "        length_lst.append((all_intervals[i][0]).shape[0])\n",
    "    return  length_lst\n",
    "\n",
    "def list_start_hour(all_intervals):\n",
    "    start_hour = []\n",
    "    for i in range(len(all_intervals)):\n",
    "        start_hour.append((all_intervals[i][1]).hour)\n",
    "    return start_hour    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Patients Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = 'path to .pkl files'\n",
    "\n",
    "InPat_Pkl_list = []\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for name in files:\n",
    "        InPat_Pkl_list.append(name)\n",
    "            \n",
    "sorted(InPat_Pkl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_lst = []\n",
    "values_lst = []\n",
    "max_len_lst = []\n",
    "min_len_lst = []\n",
    "estimated_total_num = []\n",
    "s_h_in = []\n",
    "for i in range(len(InPat_Pkl_list)):\n",
    "    with open(os.path.join(file_path, InPat_Pkl_list[i]), 'rb') as fileobj:\n",
    "        [all_intervals, num_skipped_intervals] = pickle.load(fileobj)\n",
    "        values_lst.append(num_samples(all_intervals))\n",
    "        max_len_lst.append(max_min_len(all_intervals)[1])\n",
    "        min_len_lst.append(max_min_len(all_intervals)[0])\n",
    "        length_lst.append(len_samples(all_intervals))\n",
    "        s_h_in.append(list_start_hour(all_intervals))\n",
    "        estimated_total_num.append((np.array(len_samples(all_intervals)).sum())//3600)\n",
    "        \n",
    "print(f'Total Number of Intervals:{np.array(values_lst).sum()}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Estimated Total Number of Hours:{np.array(estimated_total_num).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Number of Samples per Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_h_all = s_h_in\n",
    "s_h_lst_merged = np.array(list(itertools.chain(*s_h_all)))\n",
    "num = []\n",
    "for i in range(0,24):\n",
    "    num.append((1.0*(s_h_lst_merged==i)).sum())\n",
    "    \n",
    "h_lst = [f'0{i}:00-{i+1}:00' if i<10 else f'{i}:00-{i+1}:00' for i in range(0,24)]    \n",
    "\n",
    "# Define plot space\n",
    "fig, ax = plt.subplots(figsize=(50, 20))\n",
    "\n",
    "# Create bar plot\n",
    "ax.bar(h_lst, num)\n",
    "\n",
    "ax.set_xticklabels(h_lst, rotation=45, ha=\"right\", fontsize=30) \n",
    "plt.rc('ytick', labelsize=45)\n",
    "plt.ylabel(\"Number of samples per hour\", fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys_lst = []\n",
    "for i in range(len(InPat_Pkl_list)):\n",
    "    keys_lst.append(InPat_Pkl_list[i].split('_')[1])\n",
    "\n",
    "indices = np.argsort(keys_lst)\n",
    "indices    \n",
    "sorted_keys_lst = sorted(keys_lst)    \n",
    "sorted_estimated_total_num = [estimated_total_num[i] for i in indices]   \n",
    "t = Table([sorted_keys_lst, sorted_estimated_total_num], names=('Pat-ID', 'Total number of hours'))\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_lst_merged = list(itertools.chain(*length_lst))\n",
    "plt.figure()\n",
    "plt.hist(length_lst_merged)\n",
    "plt.xlabel(\"length\")\n",
    "plt.ylabel(\"frequencies\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2000.\n",
    "length_ = []\n",
    "for i in range(len(length_lst)):\n",
    "    length_.append(int((1.*(np.array(length_lst[i])>=threshold)).sum()))\n",
    "    \n",
    "print(f'Nubmer of Samples whose Length>={threshold}:', np.array(length_).sum()) \n",
    "print('Eligible Intervals(%):', 100*((np.array(length_).sum())/(np.array(values_lst).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding is_regular  variable to data and saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 3000\n",
    "all_data_regular = []\n",
    "all_data_irregular = []\n",
    "for i in range(len(InPat_Pkl_list)):\n",
    "    with open(os.path.join(file_path, InPat_Pkl_list[i]), 'rb') as fileobj:\n",
    "        [all_intervals, num_skipped_intervals] = pickle.load(fileobj)\n",
    "    for i in range(len(all_intervals)):\n",
    "        intervals = torch.tensor(all_intervals[i][0]) \n",
    "        s_t = all_intervals[i][1]\n",
    "        e_t = all_intervals[i][2]\n",
    "        ID = int(all_intervals[i][3])\n",
    "        start_time = [s_t.year, s_t.month, s_t.day, s_t.hour, s_t.minute]\n",
    "        end_time = [e_t.year, e_t.month, e_t.day, e_t.hour, e_t.minute]\n",
    "        if str([s_t.date(), ID]) in Dict.keys():\n",
    "            is_regular = Dict[str([s_t.date(), ID])]\n",
    "            length = len(all_intervals[i][0])\n",
    "            if length>=L:\n",
    "                if is_regular==1:\n",
    "                    all_data_regular.append([all_intervals[i][0][:L, :],\n",
    "                                     ID, is_regular, np.array(start_time), np.array(end_time)])\n",
    "                if is_regular==0:\n",
    "                    all_data_irregular.append([all_intervals[i][0][:L, :],\n",
    "                                     ID, is_regular, np.array(start_time), np.array(end_time)])    \n",
    "print(len(all_data_irregular))    \n",
    "print(len(all_data_regular)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(all_data_regular, batch_size=len(all_data_regular), shuffle=True, num_workers=0)\n",
    "x, ID, is_regular, _, _ = next(iter(loader))\n",
    "\n",
    "print(sorted(Counter(np.array(ID)).items())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(all_data_irregular, batch_size=len(all_data_irregular), shuffle=True, num_workers=0)\n",
    "x, ID, is_regular, _, _ = next(iter(loader))\n",
    "\n",
    "print(sorted(Counter(np.array(ID)).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Regular Data into train/test (90/10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [all_data_regular[i][-4] for i in range(len(all_data_regular))]\n",
    "train_set, test_set = train_test_split(all_data_regular, test_size=0.1, shuffle=True, stratify=labels)\n",
    "print('len(train_set):', len(train_set))\n",
    "print('len(test_set):', len(test_set))\n",
    "\n",
    "\n",
    "train_set_label = [train_set[i][-4] for i in range(len(train_set))]\n",
    "print(sorted(Counter(train_set_label).items()))\n",
    "\n",
    "\n",
    "test_set_label = [test_set[i][-4] for i in range(len(test_set))] \n",
    "print(sorted(Counter(test_set_label).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"path to saved data\"\n",
    "\n",
    "with open(f'{path}/train_set.pickle', 'wb') as output:\n",
    "    pickle.dump(train_set, output)\n",
    "\n",
    "with open(f'{path}/test_set.pickle', 'wb') as output:\n",
    "    pickle.dump(test_set, output) \n",
    "    \n",
    "with open(f'{path}/ood_set.pickle', 'wb') as output:\n",
    "    pickle.dump(all_data_irregular, output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
